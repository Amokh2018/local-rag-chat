{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b4164ff",
   "metadata": {},
   "source": [
    "## üîß 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5e6dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running for the first time:\n",
    "# !pip install sentence-transformers faiss-cpu PyMuPDF requests streamlit python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b2dbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import faiss\n",
    "import pickle\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8061b84",
   "metadata": {},
   "source": [
    "## üìÑ 2. Load and Chunk Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e25a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DOCS_PATH = \"data/documents/\"\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "def load_documents():\n",
    "    docs = []\n",
    "    for filename in os.listdir(DOCS_PATH):\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            path = os.path.join(DOCS_PATH, filename)\n",
    "            doc = fitz.open(path)\n",
    "            for page_no, page in enumerate(doc, start=1):\n",
    "                text = page.get_text().strip()\n",
    "                if text:\n",
    "                    docs.append({\"source\": f\"{filename} - Page {page_no}\", \"text\": text})\n",
    "    return docs\n",
    "\n",
    "def chunk_text(docs):\n",
    "    chunks = []\n",
    "    for doc in docs:\n",
    "        text = doc[\"text\"]\n",
    "        source = doc[\"source\"]\n",
    "        for i in range(0, len(text), CHUNK_SIZE - CHUNK_OVERLAP):\n",
    "            chunk = text[i:i + CHUNK_SIZE]\n",
    "            chunks.append({\"text\": chunk, \"source\": source})\n",
    "    return chunks\n",
    "\n",
    "documents = load_documents()\n",
    "chunks = chunk_text(documents)\n",
    "print(f\"Loaded {len(documents)} pages and chunked into {len(chunks)} pieces.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ae2a5f",
   "metadata": {},
   "source": [
    "## üß† 3. Build Embeddings and FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04723ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "embeddings = model.encode([chunk[\"text\"] for chunk in chunks])\n",
    "dim = embeddings[0].shape[0]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save index + chunks\n",
    "faiss.write_index(index, \"embeddings/index.faiss\")\n",
    "with open(\"embeddings/chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)\n",
    "\n",
    "print(\"‚úÖ Index built and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab0cf17",
   "metadata": {},
   "source": [
    "## üîç 4. Retrieve Relevant Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d141dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_chunks(question, top_k=3):\n",
    "    index = faiss.read_index(\"embeddings/index.faiss\")\n",
    "    with open(\"embeddings/chunks.pkl\", \"rb\") as f:\n",
    "        chunks = pickle.load(f)\n",
    "    q_embed = model.encode([question])\n",
    "    _, indices = index.search(q_embed, top_k)\n",
    "    return [chunks[i] for i in indices[0]]\n",
    "\n",
    "question = \"What is the main idea of the document?\"\n",
    "relevant_chunks = retrieve_chunks(question)\n",
    "for idx, chunk in enumerate(relevant_chunks, start=1):\n",
    "    print(f\"\\n[{chunk['source']}]:\\n{chunk['text'][:300]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109329e7",
   "metadata": {},
   "source": [
    "## üí¨ 5. Query Local LLM (via Ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c4c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OLLAMA_MODEL = \"mistral\"\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "def query_ollama(prompt):\n",
    "    payload = {\n",
    "        \"model\": OLLAMA_MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    response = requests.post(OLLAMA_URL, json=payload)\n",
    "    return response.json()[\"response\"]\n",
    "\n",
    "prompt = \"\\n\\n\".join(f\"[{c['source']}]:\\n{c['text']}\" for c in relevant_chunks)\n",
    "prompt += f\"\\n\\nQuestion:\\n{question}\"\n",
    "answer = query_ollama(prompt)\n",
    "print(\"\\nüí¨ Answer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a88725c",
   "metadata": {},
   "source": [
    "## üß© 6. Full RAG Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248b8662",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_rag(question):\n",
    "    chunks = retrieve_chunks(question)\n",
    "    prompt = \"\\n\\n\".join(f\"[{c['source']}]:\\n{c['text']}\" for c in chunks)\n",
    "    prompt += f\"\\n\\nQuestion:\\n{question}\"\n",
    "    return query_ollama(prompt)\n",
    "\n",
    "print(run_rag(\"Summarize this document in 3 lines.\"))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
