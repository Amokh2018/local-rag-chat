{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b4164ff",
   "metadata": {},
   "source": [
    "#### Install Libraries and Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5e6dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running for the first time:\n",
    "# !pip install sentence-transformers faiss-cpu PyMuPDF requests streamlit python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1b2dbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alimokh/GitHub/local-rag-chat/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import fitz  \n",
    "import os\n",
    "import faiss\n",
    "import pickle\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8061b84",
   "metadata": {},
   "source": [
    "### 2. Load and Chunk Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7caed28",
   "metadata": {},
   "source": [
    "\n",
    "![RAG Pipeline](images/indexing.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e25a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 63 pages and chunked into 568 pieces.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DOCS_PATH = \"data/documents/\"\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "def load_documents():\n",
    "    \"\"\"\n",
    "    Load PDF documents from the DOCS_PATH.\n",
    "    Returns a list of dicts with keys:\n",
    "    - 'source': a string identifier (filename and page number)\n",
    "    - 'text': the text extracted from the page\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    for filename in os.listdir(DOCS_PATH):\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            doc_path = os.path.join(DOCS_PATH, filename)\n",
    "            doc = fitz.open(doc_path)\n",
    "            for page_no, page in enumerate(doc, start=1):\n",
    "                text = page.get_text().strip()\n",
    "                if text:\n",
    "                    docs.append({\"source\": f\"{filename} - Page {page_no}\", \"text\": text})\n",
    "    return docs\n",
    "\n",
    "def chunk_text(docs):\n",
    "    \"\"\"\n",
    "    Chunk each document and preserve the source info.\n",
    "    Returns a list of chunks, each is a dict with keys:\n",
    "    - 'text': the text chunk\n",
    "    - 'source': the source info from the parent document\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for doc in docs:\n",
    "        text = doc[\"text\"]\n",
    "        source = doc[\"source\"]\n",
    "        for i in range(0, len(text), CHUNK_SIZE - CHUNK_OVERLAP):\n",
    "            chunk = text[i: i + CHUNK_SIZE]\n",
    "            chunks.append({\"text\": chunk, \"source\": source})\n",
    "    return chunks\n",
    "\n",
    "documents = load_documents()\n",
    "chunks = chunk_text(documents)\n",
    "print(f\"Loaded {len(documents)} pages and chunked into {len(chunks)} pieces.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ae2a5f",
   "metadata": {},
   "source": [
    "## 3. Build Embeddings and FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e04723ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS index created and saved.\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "INDEX_PATH = \"embeddings/index.faiss\"\n",
    "CHUNKS_PATH = \"embeddings/chunks.pkl\"\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "def build_faiss_index(chunks):\n",
    "    \"\"\"\n",
    "    Build a FAISS index from the text of the chunks.\n",
    "    Save the vector index and the mapping to chunks (with sources).\n",
    "    \"\"\"\n",
    "    # Encode using only the text\n",
    "    embeddings = model.encode([chunk[\"text\"] for chunk in chunks])\n",
    "    dim = embeddings[0].shape[0]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings)\n",
    "\n",
    "    with open(CHUNKS_PATH, \"wb\") as f:\n",
    "        pickle.dump(chunks, f)\n",
    "    faiss.write_index(index, INDEX_PATH)\n",
    "    print(\"✅ FAISS index created and saved.\")\n",
    "\n",
    "build_faiss_index(chunks)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab0cf17",
   "metadata": {},
   "source": [
    "## 🔍 4. Retrieve Relevant Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df35d58c",
   "metadata": {},
   "source": [
    "![RAG Pipeline](images/retriever.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d141dd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[AI_Chain_of_Abstraction_1709881765.pdf - Page 1]:\n",
      "the [city y2 is in -WikiSearch-> y3]. \n",
      "y1: Ralph Hefferline was a professor at \n",
      "Columbia University …\n",
      "y2: Columbia University\n",
      "y3: Columbia University is an Ivy League \n",
      "university in New York …\n",
      "The answer is New York.\n",
      "Mathematical Reasoning  \n",
      "Wiki QA  \n",
      "Figure 1: Overview of chain-of-abstraction reasoning\n",
      "with tools. Given a domain question (green scroll), a\n",
      "LLM is fine-tuned to first generate an abstract multi-step\n",
      "reasoning chain (blue bubble), and then call external\n",
      "tools to reify the chain wit...\n",
      "\n",
      "[AI_Chain_of_Abstraction_1709881765.pdf - Page 1]:\n",
      "Efficient Tool Use with Chain-of-Abstraction Reasoning\n",
      "Silin Gao1,2∗, Jane Dwivedi-Yu2, Ping Yu2, Xiaoqing Ellen Tan2,\n",
      "Ramakanth Pasunuru2, Olga Golovneva2, Koustuv Sinha2\n",
      "Asli Celikyilmaz2, Antoine Bosselut1, Tianlu Wang2\n",
      "1EPFL, 2FAIR @ Meta\n",
      "1{silin.gao,antoine.bosselut}@epfl.ch\n",
      "2{silingao,janeyu,pingyu,ellenxtan}@meta.com\n",
      "2{rpasunuru,olggol,koustuvs,aslic,tianluwang}@meta.com\n",
      "Abstract\n",
      "To achieve faithful reasoning that aligns with\n",
      "human expectations, large language models\n",
      "(LLMs) need to ground...\n",
      "\n",
      "[AI_Chain_of_Abstraction_1709881765.pdf - Page 3]:\n",
      " et al., 2021). Different from\n",
      "above work, we focus on the planning of general\n",
      "chain-of-thought (Wei et al., 2022) reasoning with\n",
      "awareness of domain specialized tools.\n",
      "3\n",
      "Method\n",
      "Chain-of-Abstraction (CoA) Reasoning\n",
      "Our\n",
      "method decouples the general reasoning of LLMs\n",
      "from domain-specific knowledge obtained from ex-\n",
      "ternal tools. Figure 1 shows an overview of our\n",
      "method. In particular, we first fine-tune LLMs to\n",
      "generate reasoning chains with abstract placehold-\n",
      "ers, e.g., y1, y2 and y3,3 as shown ...\n"
     ]
    }
   ],
   "source": [
    "def load_faiss_index():\n",
    "    \"\"\"\n",
    "    Load the FAISS index and corresponding chunks mapping.\n",
    "    \"\"\"\n",
    "    index = faiss.read_index(INDEX_PATH)\n",
    "    with open(CHUNKS_PATH, \"rb\") as f:\n",
    "        chunks = pickle.load(f)\n",
    "    return index, chunks\n",
    "\n",
    "def get_relevant_chunks(question, top_k=3):\n",
    "    \"\"\"\n",
    "    Return the top_k most relevant chunks (with their source info) based on the question.\n",
    "    \"\"\"\n",
    "    index, chunks = load_faiss_index()\n",
    "    q_embed = model.encode([question])\n",
    "    distances, indices = index.search(q_embed, top_k)\n",
    "    # Return list of dictionaries containing text and source for each retrieved chunk.\n",
    "    return [chunks[i] for i in indices[0]]\n",
    "\n",
    "question = \"Tell me about Chain of Abstraction?\"\n",
    "relevant_chunks = get_relevant_chunks(question)\n",
    "for idx, chunk in enumerate(relevant_chunks, start=1):\n",
    "    print(f\"\\n[{chunk['source']}]:\\n{chunk['text']}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109329e7",
   "metadata": {},
   "source": [
    "## 💬 5. Query Local LLM (via Ollama)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ca828b",
   "metadata": {},
   "source": [
    "![RAG Pipeline](images/RAG.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9c4c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OLLAMA_MODEL = \"mistral\"\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "def query_llm(prompt):\n",
    "    payload = {\n",
    "        \"model\": OLLAMA_MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(OLLAMA_URL, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"response\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error connecting to Ollama: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c861f749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💬 Answer:\n",
      "  The \"Chain of Abstraction\" is a method used in artificial intelligence (AI) to enable large language models (LLMs) to perform general reasoning that aligns with human expectations. This approach decouples the general reasoning of LLMs from domain-specific knowledge obtained from external tools, making it more efficient and adaptable for various tasks.\n",
      "\n",
      "In simpler terms, the Chain of Abstraction works by training an LLM to generate a series of abstract steps or placeholders, represented as \"y1\", \"y2\", and \"y3\" in this case. These placeholders can be filled with the appropriate domain-specific knowledge using external tools, allowing the model to answer complex questions effectively and accurately.\n",
      "\n",
      "References:\n",
      "1. Wei et al., 2021. Scalable Transfer Learning of Grounded Language Models. Advances in Neural Information Processing Systems (NeurIPS).\n",
      "2. Wei et al., 2022. Chain-of-Thought Reasoning: Improving Efficiency and Coherence of Large Language Models through Decoupled Planning and Execution. Proceedings of the 6th Conference on Machine Learning for Dialog Systems (WASP).\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\\n\\n\".join(f\"[{c['source']}]:\\n{c['text']}\" for c in relevant_chunks)\n",
    "prompt += f\"\\n\\nQuestion:\\n{question}\"\n",
    "answer = query_llm(prompt)\n",
    "print(\"\\n💬 Answer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a88725c",
   "metadata": {},
   "source": [
    "##  6. Full RAG Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bf7afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "INDEX_PATH = \"embeddings/index.faiss\"\n",
    "CHUNKS_PATH = \"embeddings/chunks.pkl\"\n",
    "DOCS_PATH = \"data/documents/\"\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60fbae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index():\n",
    "    print(\"📄 Loading documents...\")\n",
    "    docs = load_documents()\n",
    "\n",
    "    print(\"✂️ Chunking text...\")\n",
    "    chunks = chunk_text(docs)\n",
    "\n",
    "    print(f\"🔢 Total chunks: {len(chunks)}\")\n",
    "    print(\"🧠 Building FAISS index...\")\n",
    "    build_faiss_index(chunks)\n",
    "\n",
    "    print(\"✅ Index built and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18f479fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_TEMPLATE = \"\"\"You are a helpful assistant. Use the following context to answer the question.\n",
    "If you don't know the answer, just say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "def load_prompt_template():\n",
    "    return DEFAULT_TEMPLATE\n",
    "\n",
    "def build_prompt(chunks, question):\n",
    "    \"\"\"\n",
    "    Build a prompt by combining each chunk with its source.\n",
    "    Each chunk is annotated with the source from which it came.\n",
    "    \"\"\"\n",
    "    context_lines = []\n",
    "    for chunk in chunks:\n",
    "        # You can format the source info as you wish.\n",
    "        context_lines.append(f\"[{chunk['source']}]:\\n{chunk['text']}\")\n",
    "    context = \"\\n\\n\".join(context_lines)\n",
    "    template = load_prompt_template()\n",
    "    return template.format(context=context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cf3612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag(question):\n",
    "    print(\"🔍 Retrieving relevant chunks...\")\n",
    "    chunks = get_relevant_chunks(question)\n",
    "\n",
    "    print(\"\\n Retrieved Chunks (showing sources):\")\n",
    "    for idx, chunk in enumerate(chunks, start=1):\n",
    "        preview = chunk['text'][:100].replace(\"\\n\", \" \")  # short preview\n",
    "        print(f\"   {idx}. [{chunk['source']}]: {preview}...\")\n",
    "\n",
    "    print(\"\\nBuilding prompt with context...\")\n",
    "    prompt = build_prompt(chunks, question)\n",
    "\n",
    "    print(\"Querying local LLM...\")\n",
    "    answer = query_llm(prompt)\n",
    "\n",
    "    return answer, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03c69fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Retrieving relevant chunks...\n",
      "\n",
      " Retrieved Chunks (showing sources):\n",
      "   1. [AI_Chain_of_Abstraction_1709881765.pdf - Page 1]: the [city y2 is in -WikiSearch-> y3].  y1: Ralph Hefferline was a professor at  Columbia University ...\n",
      "   2. [AI_Chain_of_Abstraction_1709881765.pdf - Page 1]: Efficient Tool Use with Chain-of-Abstraction Reasoning Silin Gao1,2∗, Jane Dwivedi-Yu2, Ping Yu2, Xi...\n",
      "   3. [AI_Chain_of_Abstraction_1709881765.pdf - Page 3]:  et al., 2021). Different from above work, we focus on the planning of general chain-of-thought (Wei...\n",
      "\n",
      "Building prompt with context...\n",
      "Querying local LLM...\n"
     ]
    }
   ],
   "source": [
    "question = \"Tell me about Chain of Abstraction?\"\n",
    "answer, relevant_chunks = run_rag(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e65bd4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The \"Chain of Abstraction\" is a method proposed by the authors Silin Gao, Jane Dwivedi-Yu, Ping Yu, Xiaoqing Ellen Tan, Ramakanth Pasunuru, Olga Golovneva, Koustuv Sinha, Asli Celikyilmaz, Antoine Bosselut, and Tianlu Wang. This method aims to enhance the reasoning ability of large language models (LLMs) to align with human expectations.\n",
      "\n",
      "The Chain of Abstraction decouples the general reasoning of LLMs from domain-specific knowledge obtained from external tools. In this method, an LLM is fine-tuned to generate multi-step reasoning chains with abstract placeholders like y1, y2, and y3. These placeholders are then filled with information from external tools that specialize in specific domains.\n",
      "\n",
      "The goal of the Chain of Abstraction is to enable LLMs to perform faithful reasoning that takes into account both general knowledge and specialized domain knowledge. This helps improve the accuracy and relevance of the responses provided by these models, making them more useful for a wider range of tasks.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb70ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
